# Text Compression for Efficient LLMs

This project explores lossy text compression techniques to reduce input length and improve the efficiency of Large Language Models (LLMs) while preserving semantic content. Three approaches are studied: semantic pruning, cross-lingual compression, and summarization-based compression.

The methods are evaluated both quantitatively (token reduction, semantic similarity, runtime) and qualitatively by measuring their impact on downstream LLM question-answering performance. The results highlight the trade-offs between compression strength, inference speed, and semantic fidelity.
